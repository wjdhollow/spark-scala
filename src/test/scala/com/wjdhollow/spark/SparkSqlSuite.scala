/*
 * This Scala Testsuite was generated by the Gradle 'init' task.
 */
package com.wjdhollow.spark

// Scala and Java imports
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession

// Third Party Imports
import org.junit.runner.RunWith
import org.scalatest.BeforeAndAfter
import org.scalatest.BeforeAndAfterAll
import org.scalatest.flatspec.AnyFlatSpec
import org.scalatest.matchers.should.Matchers
import org.scalatestplus.junit.JUnitRunner
import org.apache.spark.sql.SaveMode

class SparkSqlSuite extends AnyFlatSpec with Matchers with BeforeAndAfterAll {
  // Constants
  val SALES_OPPORTUNITY_REL_PATH = "src/test/resources/salesOpportunity.json"
  val SALES_OPPORTUNITY_REL_ORC_PATH = "src/test/resources/salesOpportunity.orc"

  // Initialize instance variables
  val spark = SparkSession
    .builder()
    .appName("A Spark SQL app")
    .config("spark.master", "local")
    .getOrCreate()

  // Enable toDS and toDF functions
  import spark.implicits._

  // Load JSON with standard sytax
  def createJsonDataFrame(path: String): DataFrame = {
    // Default is to have each record on a line/delimited by a newline.
    // Reading elements from a JSON array is not the default setting.
    // This setting is not used in production because Spark has to
    // scan the file to detect open and closing brackets, which makes it
    // hard to parallelize. 
    spark.read.option("multiline", "true").json(path)
  }

  "toDS on a case class collection" should "create a dataset" ignore {
    val opportunities = Seq(Opportunity("fy2020Q1", 2000.00, "bestBuyIT", "bestBuy")).toDS()
    val numOpportunities = opportunities.count()
    numOpportunities should be (1)
  }

  "Reading from JSON file" should "create a Dataframe" ignore {
    val df = createJsonDataFrame(SALES_OPPORTUNITY_REL_PATH)
    val numOpportunities = df.count()
    numOpportunities should be >= 1L
  }

  "Selecting a column from a Dataframe" should "return the column" ignore {
    val df = createJsonDataFrame(SALES_OPPORTUNITY_REL_PATH)
    df.select($"dealValue").show()
  }

  "Getting a Long from a row" should "return the value" ignore {
    val df = createJsonDataFrame(SALES_OPPORTUNITY_REL_PATH)
    val dealValueTotal = df.select($"dealValue").agg(sum($"dealValue")).collect().map(_.getDouble(0)).reduce(_ + _)
    dealValueTotal should be > 10.0
  }

  "Write with orc format on JSON input" should "save as orc" in {
    val df = createJsonDataFrame(SALES_OPPORTUNITY_REL_PATH)
    // Overwrite so we can run the test multiple times. 
    df.write.mode(SaveMode.Overwrite).format("orc").save(SALES_OPPORTUNITY_REL_ORC_PATH)
  }

  override def afterAll(): Unit = {
    spark.stop()
    super.afterAll()
  }
  
}
